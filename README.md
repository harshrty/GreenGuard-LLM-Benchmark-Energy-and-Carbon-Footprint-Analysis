MLOps Benchmarks: Cost, Carbon, and Performance SimulationThis repository showcases an advanced MLOps strategy focused on quantifying the full lifecycle cost of Large Language Models (LLMs), encompassing Performance, Financial Cost, and Sustainability (Carbon Footprint).The core experiment benchmarks the Gemini 2.5 Flash model across different simulated hardware and cloud environments to identify optimal deployment configurations.üí° Project Highlights & FeaturesFeatureDescriptionMLOps ToolchainSmart Routing & A/B TestingA FastAPI server implements request routing, categorizing prompts (e.g., Flash-Precise, Flash-Coder) to apply optimal temperature settings based on task type (e.g., lower temperature for calculation, higher for creative).FastAPI, smart_router()Comprehensive ObservabilityAll API calls, including prompt, response, model type, latency, and token counts, are automatically logged for deep analysis. The data is available in the exported CSV.LiteLLM, LangfuseFull Cost of Ownership (TCO)Calculates the Total Cost of Ownership ($TCO$) by combining the model's direct API cost with the simulated energy cost of the host GPU/datacenter environment.Custom calculate_cost(), BatchProcessorSustainability BenchmarkingSimulates the environmental impact ($\text{gCO2e}$) by integrating GPU profiles, Power Usage Effectiveness ($\text{PUE}$), and regional Grid Intensity ($\text{CI}$) for detailed carbon analysis.Custom calculate_enhanced_metrics()Hardware & Location SimulationBenchmarks a single LLM across three distinct simulated hardware and cloud environments to measure the impact of infrastructure choice on final metrics.Custom GPU_PROFILES dictionaryData VisualizationGenerates detailed Plotly dashboards comparing performance metrics across different GPUs, models, and locations.Pandas, Plotly‚öôÔ∏è Benchmark MethodologyThe experiment runs 12 diverse test prompts through a Python API against the gemini/gemini-2.5-flash model.Simulated Hardware ProfilesThe core comparison is based on the simulated costs and environmental factors of three distinct environments:ProfileGPUMax Power (TDP)MemoryCloud LocationGrid Intensity (CI)PUET4_COLABNVIDIA Tesla T470W16GBGoogle Cloud US-Central (Iowa)394 $\text{gCO2e/kWh}$1.10A100_SIMULATEDNVIDIA A100 80GB400W80GBAWS US-East-1 (Virginia)340 $\text{gCO2e/kWh}$1.15L40S_SIMULATEDNVIDIA L40S350W48GBAzure West Europe (Netherlands)220 $\text{gCO2e/kWh}$1.18üèÜ Key Performance Results (27 Successful Requests)The analysis highlights significant differences in deployment efficiency based on infrastructure choice.1. Efficiency Rankings (per 1,000 Tokens)The Tesla T4 environment demonstrates the best overall efficiency for the low-latency Gemini Flash model in this specific simulation:RankMetricNVIDIA Tesla T4NVIDIA A100 80GBNVIDIA L40S1 (Cost)Total Cost/1K Tokens$0.081$0.110$0.1131 (Carbon)Carbon/1K Tokens ($\text{gCO2e}$)0.0320.1270.0902. Energy Breakdown: Idle vs. ComputeA critical finding is the dominance of non-compute energy.Total Energy Consumed: $0.001651 \text{ kWh}$Idle Energy: $0.001383 \text{ kWh}$ ($\mathbf{83.7\%}$ of total)Compute Energy: $0.000048 \text{ kWh}$ ($\mathbf{2.9\%}$ of total)Insight: For low-latency tasks like running Gemini Flash, the vast majority of energy consumption is datacenter overhead and idle power between requests. Optimization must focus on request batching and minimizing delays.3. Model Routing PerformanceThe smart_router successfully directed requests, demonstrating that task type impacts latency even on a fixed model:Fastest: Flash-Analyst (Avg. Latency: $1.124\text{s}$)Slowest: Flash-Coder (Avg. Latency: $4.184\text{s}$)üöÄ Getting Started (Run the Notebook)Dependencies: Install required libraries by running the first cell.API Keys: Provide your Gemini API Key, ngrok Authtoken, and Langfuse Keys when prompted (Cells 2-3).Execute: Run the subsequent cells to launch the FastAPI server, initiate the batch tests across the three simulated GPU environments, and generate the final report.üìÅ Repository Contentsfinal_aidevops (1).ipynb: The primary Colab notebook containing all Python code.app.py: The live application code written by the notebook, including the core API endpoints and routing logic.ai_metrics_*.csv: The final, comprehensive dataset of all 27 successful requests with financial, performance, and environmental metrics.visualizations/: Directory for the generated Plotly charts (e.g., efficiency_comparison.html).‚ö†Ô∏è Note on Financial MetricsThe calculated API Cost uses a simplified pricing formula hardcoded in the app.py script. Please note that actual, real-world Gemini API pricing may vary based on current rates, thinking tokens, and specific usage tiers. This project serves primarily as a relative comparative benchmark between hardware profiles.
